{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff65eda",
   "metadata": {},
   "source": [
    "## indlæs, lav graf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6508330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Læs data\n",
    "df = pd.read_csv(\"data/overview.csv\")\n",
    "movies_df = pd.read_csv(\"data/movies.csv\")\n",
    "\n",
    "# 2. Rens cast-kolonne\n",
    "df[\"cast_names\"] = df[\"cast_names\"].fillna(\"\").astype(str)\n",
    "\n",
    "# 3. Lav cast_list\n",
    "df[\"cast_list\"] = df[\"cast_names\"].apply(\n",
    "    lambda s: [c.strip() for c in s.split(\"|\") if c.strip() != \"\"]\n",
    ")\n",
    "\n",
    "# 4. Join overview med movies\n",
    "merged_df = df.merge(movies_df[[\"movieId\", \"genres\"]], on=\"movieId\", how=\"left\")\n",
    "\n",
    "# 5. Split genres til liste\n",
    "merged_df[\"genres\"] = merged_df[\"genres\"].fillna(\"\")\n",
    "merged_df[\"genre_list\"] = merged_df[\"genres\"].apply(\n",
    "    lambda s: [g.strip() for g in s.split(\"|\") if g.strip() != \"\"]\n",
    ")\n",
    "# tæl forekomster af hver genre\n",
    "\n",
    "\n",
    "# 6. Filtrér på en bestemt genre (præcist match)\n",
    "target_genre = \"Action\"   # ← Skift her\n",
    "\n",
    "\n",
    "genre_df = merged_df[merged_df[\"genre_list\"].apply(lambda lst: target_genre in lst) ].copy()\n",
    "\n",
    "# tag links csv filen og sørg for den kun indeholder film fra genre_df\n",
    "#links_df = pd.read_csv(\"data/links.csv\")\n",
    "#links_df = links_df[ links_df[\"movieId\"].isin( genre_df[\"movieId\"] ) ]\n",
    "#print(links_df.head())\n",
    "# giv mig links_df som csv eksporter\n",
    "#links_df.to_csv(\"data/links_action.csv\", index=False)\n",
    "# 7. Tjek resultat\n",
    "#print(genre_df.head())\n",
    "# antal film før filtrering\n",
    "print(\"Antal film i alt:\", len(merged_df))\n",
    "print(\"Antal film i genren:\", len(genre_df))\n",
    "# vis genre kolonenn\n",
    "#print(merged_df[ \"genre_list\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701efbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = list(itertools.chain.from_iterable(merged_df[\"genre_list\"]))\n",
    "genre_counts = pd.Series(all_genres).value_counts()\n",
    "print(\"Genre counts:\")\n",
    "print(genre_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx\n",
    "\n",
    "# 4. Opret graf\n",
    "G = nx.Graph()\n",
    "\n",
    "for cast in genre_df[\"cast_list\"]:\n",
    "    # Tilføj noder (skuespillere)\n",
    "    for actor in cast:\n",
    "        if actor not in G:\n",
    "            G.add_node(actor)\n",
    "    \n",
    "    # Tilføj kanter for alle par af skuespillere i samme film\n",
    "    for a, b in itertools.combinations(cast, 2):\n",
    "        if G.has_edge(a, b):\n",
    "            # øg vægten hvis kanten allerede findes\n",
    "            G[a][b][\"weight\"] += 1\n",
    "        else:\n",
    "            G.add_edge(a, b, weight=1)\n",
    "\n",
    "# 5. Fjern isolerede noder (valgfrit, men anbefales)\n",
    "H = G.copy()\n",
    "isolated = [n for n, d in H.degree() if d == 0]\n",
    "H.remove_nodes_from(isolated)\n",
    "\n",
    "# Info\n",
    "print(\"Original graf: {} noder, {} kanter\".format(G.number_of_nodes(), G.number_of_edges()))\n",
    "print(\"Renset graf uden isolerede noder: {} noder, {} kanter\".format(H.number_of_nodes(), H.number_of_edges()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9df41",
   "metadata": {},
   "source": [
    "## Graf undersøgelse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc788a32",
   "metadata": {},
   "source": [
    "- Degree Distribtuion\n",
    "- Hvem har lavest og højest degree distribution\n",
    "- Max/min degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree distribution plot\n",
    "degrees = [d for n, d in H.degree()]\n",
    "plt.hist(degrees, bins=range(1, max(degrees)+1), edgecolor='black')\n",
    "plt.title(\"Degree Distribution\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.yscale('log')  # Log-skala for bedre visualisering\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebccd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Højst og lavest degree\n",
    "degree_sequence = sorted(H.degree(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 skuespillere efter degree:\")\n",
    "for actor, degree in degree_sequence[:5]:\n",
    "    print(f\"{actor}: {degree}\")\n",
    "print(\"\\nBottom 5 skuespillere efter degree:\")\n",
    "for actor, degree in degree_sequence[-5:]:\n",
    "    print(f\"{actor}: {degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max og min degree\n",
    "max_degree_actor, max_degree = degree_sequence[0]\n",
    "min_degree_actor, min_degree = degree_sequence[-1]\n",
    "print(\"\\nSkuespiller med højst degree: {} ({})\".format(max_degree_actor, max_degree))\n",
    "print(\"Skuespiller med lavest degree: {} ({})\".format(min_degree_actor, min_degree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285e140",
   "metadata": {},
   "source": [
    "## Community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b031c",
   "metadata": {},
   "source": [
    "### Community TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb37cb",
   "metadata": {},
   "source": [
    "- Modularity\n",
    "- Structural Communities (Louvain algorithm) to find optimal structural communities\n",
    "- Purity metrics - Community purity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find GCC\n",
    "components = nx.connected_components(H)\n",
    "gcc_nodes = max(components, key=len)\n",
    "\n",
    "# Lav en subgraf for GCC\n",
    "H_gcc = H.subgraph(gcc_nodes).copy()\n",
    "\n",
    "print(f\"Størrelse af GCC: {H_gcc.number_of_nodes()} nodes, {H_gcc.number_of_edges()} edges\")\n",
    "\n",
    "# Kør Louvain på GCC\n",
    "from community import community_louvain\n",
    "\n",
    "partition_gcc = community_louvain.best_partition(H_gcc, weight=\"weight\")\n",
    "\n",
    "# Antal communities\n",
    "num_comms = len(set(partition_gcc.values()))\n",
    "print(f\"Louvain på GCC fandt {num_comms} communities\")\n",
    "\n",
    "# Modularity\n",
    "Q = community_louvain.modularity(partition_gcc, H_gcc)\n",
    "print(f\"Modularity på GCC: {Q:.4f}\")\n",
    "from collections import Counter\n",
    "\n",
    "community_sizes_gcc = Counter(partition_gcc.values())\n",
    "\n",
    "# Sorter fra største til mindste\n",
    "for comm, size in community_sizes_gcc.most_common(20):\n",
    "    print(f\"Community {comm}: {size} skuespillere\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddf585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "community_names = {}\n",
    "community_hubs = {}\n",
    "\n",
    "# Find top-1 hubs + navngiv\n",
    "for comm in set(partition_gcc.values()):\n",
    "    # noder i community\n",
    "    nodes = [n for n, c in partition_gcc.items() if c == comm]\n",
    "    sub = H_gcc.subgraph(nodes)\n",
    "    \n",
    "    # weighted degree\n",
    "    degrees = sub.degree(weight=\"weight\")\n",
    "    \n",
    "    # sortér efter degree (højeste først)\n",
    "    top1 = sorted(degrees, key=lambda x: x[1], reverse=True)[:1]\n",
    "    community_hubs[comm] = top1\n",
    "    \n",
    "    # lav community-navn ud fra top-1\n",
    "    hub_names = [actor for actor, deg in top1]\n",
    "    community_name = \" - \".join(hub_names)\n",
    "    \n",
    "    community_names[comm] = community_name\n",
    "\n",
    "# Udskriv de 20 største communities med auto-navn\n",
    "from collections import Counter\n",
    "sizes = Counter(partition_gcc.values())\n",
    "\n",
    "print(\"Top 20 communities med navne:\")\n",
    "for comm, size in sizes.most_common(20):\n",
    "    print(f\"Community {comm} (size {size}) → {community_names[comm]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_overview = dict(zip(genre_df[\"movieId\"], genre_df[\"overview\"]))\n",
    "from collections import defaultdict\n",
    "\n",
    "actor_movies = defaultdict(list)\n",
    "\n",
    "for movie_id, cast in zip(genre_df[\"movieId\"], genre_df[\"cast_list\"]):\n",
    "    for actor in cast:\n",
    "        actor_movies[actor].append(movie_id)\n",
    "\n",
    "print(actor_movies[\"Bruce Willis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa02673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "community_corpus = defaultdict(list)\n",
    "\n",
    "for actor, comm in partition_gcc.items():\n",
    "    for movie_id in actor_movies.get(actor, []):\n",
    "        overview = movie_overview.get(movie_id, \"\")\n",
    "        if isinstance(overview, str) and len(overview.strip()) > 0:\n",
    "            community_corpus[comm].append(overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6dfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "\n",
    "community_keywords = {}\n",
    "\n",
    "for comm, docs in community_corpus.items():\n",
    "    if len(docs) < 10:  # skip tiny communities\n",
    "        continue\n",
    "    \n",
    "    tfidf = vectorizer.fit_transform(docs)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    avg_scores = tfidf.mean(axis=0).A1\n",
    "    top_idx = avg_scores.argsort()[-15:][::-1]\n",
    "    \n",
    "    keywords = [features[i] for i in top_idx]\n",
    "    community_keywords[comm] = keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_communities = [65, 54, 3, 9, 11, 25, 32, 4, 47, 63]\n",
    "\n",
    "for comm in top_communities:\n",
    "    print(f\"\\nCommunity {comm}: {community_names[comm]}\")\n",
    "    print(\"Keywords:\")\n",
    "    print(\", \".join(community_keywords.get(comm, [\"No data\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab5731",
   "metadata": {},
   "source": [
    "### Sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc32cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load LabMT data\n",
    "labmt = pd.read_csv(\"Data_Set_S1.txt\", sep=\"\\t\")\n",
    "\n",
    "# Dictionary: ord → happiness score\n",
    "labmt_dict = dict(zip(\n",
    "    labmt[\"word\"].astype(str).str.lower(),\n",
    "    labmt[\"happiness_average\"]\n",
    "))\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"[a-z']+\", str(text).lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ca37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labmt_sentiment(text, word_dict, lens=1.0, center=5.0):\n",
    "    tokens = tokenize(text)\n",
    "    scores = [word_dict[t] for t in tokens if t in word_dict]\n",
    "\n",
    "    # fjern neutrale ord (fx \"movie\", \"film\", \"time\", \"man\"...)\n",
    "    filtered = [s for s in scores if s < center - lens or s > center + lens]\n",
    "\n",
    "    if not filtered:\n",
    "        return None\n",
    "\n",
    "    return float(np.mean(filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d908b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv(\"data/reviews.csv\")\n",
    "\n",
    "reviews_df[\"sentiment\"] = reviews_df[\"reviews\"].apply(\n",
    "    lambda text: labmt_sentiment(text, labmt_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_sentiment = (\n",
    "    reviews_df.groupby(\"movieId\")[\"sentiment\"].mean().to_dict()\n",
    ")\n",
    "\n",
    "# fjern alle NaN værdier\n",
    "movie_sentiment = {m: s for m, s in movie_sentiment.items() if s == s}\n",
    "\n",
    "print(movie_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647351d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "actor_sentiment = {}\n",
    "\n",
    "for actor, movies in actor_movies.items():\n",
    "    vals = []\n",
    "    for m in movies:\n",
    "        if m in movie_sentiment:\n",
    "            if not np.isnan(movie_sentiment[m]):   # eksplicit nan-check\n",
    "                vals.append(movie_sentiment[m])\n",
    "    \n",
    "    actor_sentiment[actor] = np.mean(vals) if len(vals) > 0 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f59fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actor_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "community_sentiment = defaultdict(list)\n",
    "\n",
    "for actor, comm in partition_gcc.items():\n",
    "    if actor_sentiment[actor] is not None:\n",
    "        community_sentiment[comm].append(actor_sentiment[actor])\n",
    "\n",
    "community_sentiment_mean = {\n",
    "    comm: np.mean(vals) for comm, vals in community_sentiment.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sent = sorted(\n",
    "    community_sentiment_mean.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "sorted_sent[:10]   # top 10 mest positive communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = []\n",
    "\n",
    "for comm, sent in sorted_sent[:10]:\n",
    "    name = community_names.get(comm, \"Unknown\")\n",
    "    linked.append({\n",
    "        \"community\": comm,\n",
    "        \"name\": name,\n",
    "        \"sentiment\": sent\n",
    "    })\n",
    "\n",
    "linked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6038056c",
   "metadata": {},
   "source": [
    "## Network Modellering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ad228",
   "metadata": {},
   "source": [
    "- Small World\n",
    "- Scale Free\n",
    "- Power Law Exponent\n",
    "- Bipartite Network (anti block diagonals and perform projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f306cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Largest connected component (LCC)\n",
    "Hcc = H.subgraph(max(nx.connected_components(H), key=len)).copy()\n",
    "\n",
    "print(\"LCC nodes:\", Hcc.number_of_nodes())\n",
    "print(\"LCC edges:\", Hcc.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5aa1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_H = nx.average_shortest_path_length(Hcc)\n",
    "C_H = nx.average_clustering(Hcc)\n",
    "\n",
    "print(\"Average shortest path length (H):\", L_H)\n",
    "print(\"Average clustering (H):\", C_H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = Hcc.number_of_nodes()\n",
    "M = Hcc.number_of_edges()\n",
    "\n",
    "H_er = nx.gnm_random_graph(N, M)\n",
    "\n",
    "L_er = nx.average_shortest_path_length(H_er)\n",
    "C_er = nx.average_clustering(H_er)\n",
    "\n",
    "print(\"ER shortest path:\", L_er)\n",
    "print(\"ER clustering:\", C_er)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5e701",
   "metadata": {},
   "source": [
    "## Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b41932",
   "metadata": {},
   "source": [
    "- Top 5 nodes for Degree Centrality\n",
    "- Top 5 nodes for Betweenness Centrality\n",
    "- Top 5 nodes for Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "sorted_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 skuespillere efter degree centrality:\")\n",
    "for actor, centrality in sorted_degree[:5]:\n",
    "    print(f\"{actor}: {centrality:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446696a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "sorted_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 skuespillere efter betweenness centrality:\")\n",
    "for actor, centrality in sorted_betweenness[:5]:\n",
    "    print(f\"{actor}: {centrality:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afce869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvector centrality\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "sorted_eigenvector = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 skuespillere efter eigenvector centrality:\")\n",
    "for actor, centrality in sorted_eigenvector[:5]:\n",
    "    print(f\"{actor}: {centrality:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae75b1f",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158b1ff",
   "metadata": {},
   "source": [
    "- Spearman\n",
    "- Pearson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman korrelation mellem centralitetsmålene\n",
    "from scipy.stats import spearmanr\n",
    "degree_values = [v for k, v in sorted_degree]\n",
    "betweenness_values = [v for k, v in sorted_betweenness]\n",
    "eigenvector_values = [v for k, v in sorted_eigenvector]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf14ec0",
   "metadata": {},
   "source": [
    "## Assortativity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3aaba3",
   "metadata": {},
   "source": [
    "- Degree Assortativity\n",
    "- Attribute Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree assortativity\n",
    "assortativity = nx.degree_assortativity_coefficient(G)\n",
    "print(\"\\nDegree assortativity koefficient:\", assortativity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870cae73",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcf300",
   "metadata": {},
   "source": [
    "- Layouts (spring_layout vs kamada_kawai_layout)\n",
    "- ForceAtlas2 for aestethic visualizations\n",
    "- Node size depending on degree\n",
    "- Heatmaps, plot of in and out degrees\n",
    "- Backbone - Displarity Filter & High-Salience Skeleteon (HSS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "def disparity_filter(G, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Extract backbone using the Disparity Filter method.\n",
    "    G: weighted NetworkX graph\n",
    "    alpha: significance threshold (default 0.05)\n",
    "    Returns: backbone graph (NetworkX)\n",
    "    \"\"\"\n",
    "    backbone = nx.Graph()\n",
    "    backbone.add_nodes_from(G.nodes(data=True))\n",
    "\n",
    "    for node in G.nodes():\n",
    "        k = len(list(G.neighbors(node)))     # degree\n",
    "        if k <= 1:\n",
    "            # keep all edges for nodes with degree 1\n",
    "            for nbr, data in G[node].items():\n",
    "                backbone.add_edge(node, nbr, **data)\n",
    "            continue\n",
    "\n",
    "        # sum of incident weights\n",
    "        w_sum = sum(data[\"weight\"] for _, data in G[node].items())\n",
    "\n",
    "        for nbr, data in G[node].items():\n",
    "            w = data[\"weight\"]\n",
    "            p_ij = w / w_sum\n",
    "\n",
    "            # Disparity Filter significance test\n",
    "            alpha_ij = 1 - (1 - p_ij) ** (k - 1)\n",
    "\n",
    "            if alpha_ij < alpha:\n",
    "                # keep significant edge\n",
    "                backbone.add_edge(node, nbr, **data)\n",
    "\n",
    "    return backbone\n",
    "\n",
    "# Anvend disparity filter på grafen H\n",
    "backbone_H = disparity_filter(H, alpha=0.5)\n",
    "print(\"\\nBackbone graf efter Disparity Filter: {} noder, {} kanter\".format(backbone_H.number_of_nodes(), backbone_H.number_of_edges()))\n",
    "\n",
    "# plot\n",
    "pos = nx.spring_layout(backbone_H, seed=42)\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx_nodes(backbone_H, pos, node_size=20, node_color='blue')\n",
    "nx.draw_networkx_edges(backbone_H, pos, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c33ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "def high_salience_skeleton(G, threshold=0.1, weight=\"weight\"):\n",
    "    \"\"\"\n",
    "    Extract backbone using the High Salience Skeleton method.\n",
    "    \n",
    "    G: weighted NetworkX graph\n",
    "    threshold: salience threshold (0–1)\n",
    "    weight: edge attribute used as distance\n",
    "    \n",
    "    Returns: backbone graph (NetworkX)\n",
    "    \"\"\"\n",
    "    # Prepare salience counter\n",
    "    salience = defaultdict(int)\n",
    "    N = len(G.nodes())\n",
    "\n",
    "    # Compute all-pairs shortest paths\n",
    "    # For large graphs: this is expensive (O(N^3))\n",
    "    for source in G.nodes():\n",
    "        paths = nx.single_source_dijkstra_path(G, source, weight=weight)\n",
    "\n",
    "        # Count edge usage\n",
    "        for target, path in paths.items():\n",
    "            if len(path) < 2:\n",
    "                continue\n",
    "            \n",
    "            # For each edge in this shortest path, count usage\n",
    "            for u, v in zip(path[:-1], path[1:]):\n",
    "                if u < v:   # undirected normalization\n",
    "                    salience[(u, v)] += 1\n",
    "                else:\n",
    "                    salience[(v, u)] += 1\n",
    "\n",
    "    # Total number of paths = N*(N-1) / 2 for undirected\n",
    "    total_paths = N * (N - 1) / 2\n",
    "\n",
    "    # Build backbone graph\n",
    "    backbone = nx.Graph()\n",
    "    backbone.add_nodes_from(G.nodes(data=True))\n",
    "\n",
    "    for (u, v), count in salience.items():\n",
    "        S = count / total_paths   # salience score\n",
    "        \n",
    "        if S >= threshold:\n",
    "            # copy attributes from original graph\n",
    "            data = G[u][v]\n",
    "            backbone.add_edge(u, v, **data, salience=S)\n",
    "\n",
    "    return backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c84df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_HSS = high_salience_skeleton(H, threshold=0.0005)\n",
    "# Plot High Salience Skeleton backbone\n",
    "pos = nx.spring_layout(backbone_HSS, seed=42)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx_nodes(backbone_HSS, pos, node_size=20, node_color='red')\n",
    "nx.draw_networkx_edges(backbone_HSS, pos, alpha=0.5)\n",
    "\n",
    "plt.title(\"Backbone Graph efter High Salience Skeleton\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9024ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocknet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
