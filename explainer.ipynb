{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0443c9f4",
   "metadata": {},
   "source": [
    "# Action Network â€” Group 97  \n",
    "### Social Graphs & Interactions\n",
    "\n",
    "![Project Cover Image](https://w0.peakpx.com/wallpaper/194/584/HD-wallpaper-skyscraper-movie-10k-skyscraper-movie-2018-movies-movies-dwayne-johnson.jpg)\n",
    "\n",
    "ðŸ”— **GitHub Repository:**  \n",
    "[Click here to view the repository](INSERT_GITHUB_LINK_HERE)\n",
    "\n",
    "---\n",
    "\n",
    "## Welcome to Our Project Explainer!\n",
    "\n",
    "This Jupyter Notebook contains **all the code and explanations** related to the final project for **Group 97** in the course *Social Graphs & Interactions*.\n",
    "\n",
    "Each section of the notebook is structured to guide you through:\n",
    "- The **purpose** of the code  \n",
    "- The **implementation details**  \n",
    "- And **how everything ties together**\n",
    "\n",
    "ðŸ’¡ Throughout the notebook, you will find **clear comments and explanations** for every major part of the code to ensure transparency and easy understanding.\n",
    "\n",
    "---\n",
    "\n",
    "**How to Use This Notebook**\n",
    "- Expand the sections below to explore each part of the project  \n",
    "- Follow the comments inside the code cells for step-by-step explanations  \n",
    "- Use the GitHub link above for the full project structure, version control, and documentation\n",
    "\n",
    "---\n",
    "\n",
    "Enjoy exploring our work â€” and feel free to reach out if you have questions! ðŸ™Œ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea85887",
   "metadata": {},
   "source": [
    "# Scraping section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87803e84",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3162df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from community import community_louvain\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6f068",
   "metadata": {},
   "source": [
    "## Function for scraping and storing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70c9b3",
   "metadata": {},
   "source": [
    "This section handles the automated retrieval and storage of movie reviews from the TMDB API.\n",
    "\n",
    "1. **Fetching review data from TMDB**  \n",
    "   The function `fetch_movie_data` sends a request to the TMDB API for a given movie ID and returns the corresponding review data in JSON format. A brief delay is added periodically to avoid exceeding rate limits.\n",
    "\n",
    "2. **Appending review data to a CSV file**  \n",
    "   The function `write_to_csv` appends processed review information to a CSV file used to store all collected reviews.\n",
    "\n",
    "3. **Resuming progress based on last processed movie**  \n",
    "   The function `get_last_row_movie` reads the last row of the review file to determine which movie ID was most recently processed.  \n",
    "   The main processing function uses this value to resume data collection without repeating previously completed work.\n",
    "\n",
    "4. **Processing movie metadata and extracting review text**  \n",
    "   For each movie, the script checks whether it has already been processed and whether it has a valid TMDB ID.  \n",
    "   If valid, reviews are fetched, and the first few reviews are extracted, cleaned, and concatenated into a single string.\n",
    "\n",
    "5. **Saving processed reviews**  \n",
    "   Each movieâ€™s cleaned review text is stored along with its movie ID in the output CSV file.  \n",
    "   This ensures that the dataset can be built incrementally and used later for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdaf302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch movie reviews from the TMDB API\n",
    "def fetch_movie_data(tmdb_id, i, TMDB_API_KEY):\n",
    "    if i % 39 == 0:\n",
    "        time.sleep(0.25)\n",
    "\n",
    "    url = (\n",
    "        f\"https://api.themoviedb.org/3/movie/{tmdb_id}/reviews\"\n",
    "        f\"?api_key={TMDB_API_KEY}&language=en-US\"\n",
    "    )\n",
    "\n",
    "    data = requests.get(url).json()\n",
    "    return data\n",
    "\n",
    "\n",
    "# Append a row to a CSV file\n",
    "def write_to_csv(row, filename=\"ml-latest/reviews.csv\"):\n",
    "    with open(filename, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "# Get the last processed movieId from the CSV file\n",
    "def get_last_row_movie(filename=\"ml-latest/reviews.csv\"):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        last_line = f.readlines()[-1]\n",
    "        return int(last_line.split(\",\")[0])\n",
    "\n",
    "\n",
    "# Fetch, process, and store movie reviews\n",
    "def process_movie_data(data, TMDB_API_KEY, file_path=\"ml-latest/reviews.csv\"):\n",
    "\n",
    "    # Check if output file exists\n",
    "    if os.path.exists(file_path):\n",
    "        last_movie_id = get_last_row_movie(file_path)\n",
    "        print(f\"Resuming from movieId: {last_movie_id}\")\n",
    "    else:\n",
    "        last_movie_id = 0\n",
    "        with open(file_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"movieId\", \"reviews\"])\n",
    "        print(\"Creating new reviews.csv file\")\n",
    "\n",
    "    for i, (movieId, imdbId, tmdbId) in enumerate(data.itertuples(index=False)):\n",
    "\n",
    "        # Skip already processed movies\n",
    "        if movieId <= last_movie_id:\n",
    "            continue\n",
    "\n",
    "        # Skip missing TMDB IDs\n",
    "        if pd.isna(tmdbId):\n",
    "            continue\n",
    "\n",
    "        tmdbId = int(tmdbId)\n",
    "\n",
    "        # Fetch review data\n",
    "        review_data = fetch_movie_data(tmdbId, i, TMDB_API_KEY)\n",
    "\n",
    "        # Extract and clean review texts\n",
    "        reviews_raw = review_data.get(\"results\", [])\n",
    "        reviews_texts = []\n",
    "\n",
    "        for r in reviews_raw[:5]:\n",
    "            content = r.get(\"content\", \"\")\n",
    "            content = content.replace(\"\\n\", \" \").replace(\",\", \"\").replace(\"'\", \"\")\n",
    "            reviews_texts.append(content)\n",
    "\n",
    "        reviews_joined = \"|\".join(reviews_texts)\n",
    "\n",
    "        # Save result to CSV\n",
    "        row = [movieId, reviews_joined]\n",
    "        write_to_csv(row, file_path)\n",
    "\n",
    "        print(f\"Saved movieId {movieId}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "TMDB_API_KEY = \"be0552b72397e07ffaa4d7d488b22b92\"\n",
    "\n",
    "movies_df = pd.read_csv(\"data/links_action.csv\")\n",
    "process_movie_data(movies_df, TMDB_API_KEY=TMDB_API_KEY, file_path=\"data/reviews_final_one_AA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63372c3",
   "metadata": {},
   "source": [
    "# Graph section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7c05a",
   "metadata": {},
   "source": [
    "## Initialization of data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c03d2",
   "metadata": {},
   "source": [
    "This section prepares the movie data and constructs the actor collaboration network.\n",
    "\n",
    "1. **Load and clean datasets**  \n",
    "   The overview and movie metadata datasets are loaded.  \n",
    "   The cast column is cleaned by filling missing values and converting all entries to strings.\n",
    "\n",
    "2. **Convert cast and genre data to list format**  \n",
    "   Cast names are split into lists of individual actors.  \n",
    "   Genres are split into lists of individual genre labels to allow flexible filtering.\n",
    "\n",
    "3. **Merge datasets on movie ID**  \n",
    "   The overview data is merged with the genre information using `movieId` as the key.\n",
    "\n",
    "4. **Filter movies by target genre**  \n",
    "   The dataset is filtered to include only movies belonging to the selected genre (Action).\n",
    "\n",
    "5. **Construct the actor network**  \n",
    "   A graph is created where each actor is a node.  \n",
    "   Edges are added between actors who appear in the same movie, with edge weights representing repeated collaborations.\n",
    "\n",
    "6. **Remove isolated nodes**  \n",
    "   Actors with no connections are removed to ensure the network only contains collaborative relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bebad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df = pd.read_csv(\"data/overview.csv\")\n",
    "movies_df = pd.read_csv(\"data/movies.csv\")\n",
    "\n",
    "# Clean cast column\n",
    "df[\"cast_names\"] = df[\"cast_names\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Convert cast strings to lists\n",
    "df[\"cast_list\"] = df[\"cast_names\"].apply(\n",
    "    lambda s: [c.strip() for c in s.split(\"|\") if c.strip() != \"\"]\n",
    ")\n",
    "\n",
    "# Merge overview with genre data\n",
    "merged_df = df.merge(movies_df[[\"movieId\", \"genres\"]], on=\"movieId\", how=\"left\")\n",
    "\n",
    "# Prepare genre column\n",
    "merged_df[\"genres\"] = merged_df[\"genres\"].fillna(\"\")\n",
    "merged_df[\"genre_list\"] = merged_df[\"genres\"].apply(\n",
    "    lambda s: [g.strip() for g in s.split(\"|\") if g.strip() != \"\"]\n",
    ")\n",
    "\n",
    "# Filter by target genre\n",
    "target_genre = \"Action\"\n",
    "genre_df = merged_df[\n",
    "    merged_df[\"genre_list\"].apply(lambda lst: target_genre in lst)\n",
    "].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d03c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph\n",
    "G = nx.Graph()\n",
    "\n",
    "for cast in genre_df[\"cast_list\"]:\n",
    "    # Add actors as nodes\n",
    "    for actor in cast:\n",
    "        if actor not in G:\n",
    "            G.add_node(actor)\n",
    "    \n",
    "    # Add edges between actors appearing in the same movie\n",
    "    for a, b in itertools.combinations(cast, 2):\n",
    "        if G.has_edge(a, b):\n",
    "            G[a][b][\"weight\"] += 1  # increase weight if edge already exists\n",
    "        else:\n",
    "            G.add_edge(a, b, weight=1)\n",
    "\n",
    "H = G.copy()\n",
    "isolated = [n for n, d in H.degree() if d == 0]\n",
    "H.remove_nodes_from(isolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681efab",
   "metadata": {},
   "source": [
    "## Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c763a7",
   "metadata": {},
   "source": [
    "\n",
    "This section evaluates the importance of actors in the network using three different centrality measures. Each measure highlights a different aspect of influence or prominence within the actor collaboration graph.\n",
    "\n",
    "1. **Degree centrality**  \n",
    "   Measures how many direct connections each actor has.  \n",
    "   Actors with high degree centrality have collaborated with many others and are therefore highly embedded in the network.  \n",
    "   The code computes degree centrality for all nodes and prints the top five actors with the most connections.\n",
    "\n",
    "2. **Betweenness centrality**  \n",
    "   Measures how often a node lies on the shortest paths between other nodes.  \n",
    "   Actors with high betweenness centrality serve as bridges between otherwise separate parts of the network and may play an important structural role.  \n",
    "   The code calculates betweenness centrality and outputs the top five actors acting as key intermediaries.\n",
    "\n",
    "3. **Eigenvector centrality**  \n",
    "   Measures influence by considering not only how many connections an actor has but also the importance of the actors they are connected to.  \n",
    "   High eigenvector centrality indicates that an actor is connected to other highly influential actors.  \n",
    "   The code computes eigenvector centrality and lists the top five most influential actors under this measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree centrality for all nodes\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Sort actors by centrality score\n",
    "sorted_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top 5 actors\n",
    "print(\"\\nTop 5 skuespillere efter degree centrality:\")\n",
    "for actor, centrality in sorted_degree[:5]:\n",
    "    print(f\"{actor}: {centrality:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b967149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate betweenness centrality for all nodes\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Sort actors by betweenness score\n",
    "sorted_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top 5 actors\n",
    "print(\"\\nTop 5 skuespillere efter betweenness centrality:\")\n",
    "for actor, centrality in sorted_betweenness[:5]:\n",
    "    print(f\"{actor}: {centrality:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate eigenvector centrality for all nodes\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "\n",
    "# Sort actors by eigenvector score\n",
    "sorted_eigenvector = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top 5 actors\n",
    "print(\"\\nTop 5 skuespillere efter eigenvector centrality:\")\n",
    "for actor, centrality in sorted_eigenvector[:5]:\n",
    "    print(f\"{actor}: {centrality:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685641d1",
   "metadata": {},
   "source": [
    "## Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06c6e8",
   "metadata": {},
   "source": [
    "In this part of the notebook, we detect and interpret communities in the actor network. The code does the following:\n",
    "\n",
    "1. **Extract the Giant Connected Component (GCC)**  \n",
    "   From the cleaned graph `H`, the largest connected component is selected and stored as `H_gcc`. All further analysis is restricted to this subgraph.\n",
    "\n",
    "2. **Run Louvain community detection on the GCC**  \n",
    "   The Louvain algorithm is applied to `H_gcc` (using edge weights as collaboration strength).  \n",
    "   This produces a partition `partition_gcc` that assigns each actor to a community.  \n",
    "   The number of communities and the modularity score are computed to assess the structure.\n",
    "\n",
    "3. **Measure and rank community sizes**  \n",
    "   The size (number of actors) of each community is counted, and the largest communities are identified.\n",
    "\n",
    "4. **Identify hub actors and name communities**  \n",
    "   For each community, a subgraph is created and the weighted degree of each actor is computed.  \n",
    "   The top actor by degree is selected as the hub, and the community is given an automatic name based on this hub.\n",
    "\n",
    "5. **Build a text corpus per community**  \n",
    "   Using `actor_movies` and `movie_overview`, all movie overviews associated with actors in each community are collected into `community_corpus`.\n",
    "\n",
    "6. **Extract keywords with TFâ€“IDF**  \n",
    "   For communities with enough text (at least 10 overviews), a TFâ€“IDF model is fitted.  \n",
    "   The average TFâ€“IDF score per term is computed, and the top keywords are selected as `community_keywords[comm]`.\n",
    "\n",
    "7. **Present selected communities**  \n",
    "   For a chosen list of communities, the code prints the community ID, its automatically generated hub-based name, and the main TFâ€“IDF keywords that characterize the movies associated with that community.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf47d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract connected components\n",
    "components = nx.connected_components(H)\n",
    "\n",
    "# Select the largest connected component\n",
    "gcc_nodes = max(components, key=len)\n",
    "\n",
    "# Create subgraph of the giant connected component\n",
    "H_gcc = H.subgraph(gcc_nodes).copy()\n",
    "\n",
    "# Print size of the GCC\n",
    "print(f\"Size of GCC: {H_gcc.number_of_nodes()} nodes, {H_gcc.number_of_edges()} edges\")\n",
    "\n",
    "# Apply Louvain community detection on the GCC\n",
    "partition_gcc = community_louvain.best_partition(H_gcc, weight=\"weight\")\n",
    "\n",
    "# Count number of detected communities\n",
    "num_comms = len(set(partition_gcc.values()))\n",
    "print(f\"Louvain on GCC found {num_comms} communities\")\n",
    "\n",
    "# Compute modularity of the partition\n",
    "Q = community_louvain.modularity(partition_gcc, H_gcc)\n",
    "print(f\"Modularity on GCC: {Q:.4f}\")\n",
    "\n",
    "# Count community sizes\n",
    "community_sizes_gcc = Counter(partition_gcc.values())\n",
    "\n",
    "# Print the largest communities\n",
    "for comm, size in community_sizes_gcc.most_common(20):\n",
    "    print(f\"Community {comm}: {size} actors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2469e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_names = {}\n",
    "community_hubs = {}\n",
    "\n",
    "# Find top hub and assign community name\n",
    "for comm in set(partition_gcc.values()):\n",
    "    # Get nodes in the community\n",
    "    nodes = [n for n, c in partition_gcc.items() if c == comm]\n",
    "    sub = H_gcc.subgraph(nodes)\n",
    "    \n",
    "    # Compute weighted degrees\n",
    "    degrees = sub.degree(weight=\"weight\")\n",
    "    \n",
    "    # Select top-1 node by degree\n",
    "    top1 = sorted(degrees, key=lambda x: x[1], reverse=True)[:1]\n",
    "    community_hubs[comm] = top1\n",
    "    \n",
    "    # Create community name from top hub\n",
    "    hub_names = [actor for actor, deg in top1]\n",
    "    community_name = \" - \".join(hub_names)\n",
    "    \n",
    "    community_names[comm] = community_name\n",
    "\n",
    "# Count community sizes\n",
    "sizes = Counter(partition_gcc.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e74868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map movie IDs to their overviews\n",
    "movie_overview = dict(zip(genre_df[\"movieId\"], genre_df[\"overview\"]))\n",
    "\n",
    "# Store movies for each actor\n",
    "actor_movies = defaultdict(list)\n",
    "\n",
    "# Assign movie IDs to each actor based on cast lists\n",
    "for movie_id, cast in zip(genre_df[\"movieId\"], genre_df[\"cast_list\"]):\n",
    "    for actor in cast:\n",
    "        actor_movies[actor].append(movie_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect movie overviews for each community\n",
    "community_corpus = defaultdict(list)\n",
    "\n",
    "for actor, comm in partition_gcc.items():\n",
    "    for movie_id in actor_movies.get(actor, []):\n",
    "        overview = movie_overview.get(movie_id, \"\")\n",
    "        if isinstance(overview, str) and len(overview.strip()) > 0:\n",
    "            community_corpus[comm].append(overview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a7a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "\n",
    "community_keywords = {}\n",
    "\n",
    "for comm, docs in community_corpus.items():\n",
    "    # Skip very small communities\n",
    "    if len(docs) < 10:\n",
    "        continue\n",
    "    \n",
    "    # Compute TF-IDF matrix\n",
    "    tfidf = vectorizer.fit_transform(docs)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Compute average TF-IDF scores\n",
    "    avg_scores = tfidf.mean(axis=0).A1\n",
    "    top_idx = avg_scores.argsort()[-15:][::-1]\n",
    "    \n",
    "    # Extract top keywords\n",
    "    keywords = [features[i] for i in top_idx]\n",
    "    community_keywords[comm] = keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific top communities\n",
    "top_communities = [65, 54, 3, 9, 11, 25, 32, 4, 47, 63]\n",
    "\n",
    "# Print community names and keywords\n",
    "for comm in top_communities:\n",
    "    print(f\"\\nCommunity {comm}: {community_names[comm]}\")\n",
    "    print(\"Keywords:\")\n",
    "    print(\", \".join(community_keywords.get(comm, [\"No data\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8604fb3a",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59775d",
   "metadata": {},
   "source": [
    "In this section, sentiment scores are computed and aggregated from reviews to actors and communities.\n",
    "\n",
    "1. **Load and process movie reviews**  \n",
    "   Movie reviews are loaded and a LabMT-based sentiment score is computed for each review using the `labmt_sentiment` function.\n",
    "\n",
    "2. **Aggregate sentiment at movie level**  \n",
    "   The mean sentiment score is computed for each movie by grouping reviews by `movieId`.  \n",
    "   All missing (NaN) values are removed.\n",
    "\n",
    "3. **Aggregate sentiment at actor level**  \n",
    "   For each actor, the sentiment scores of all movies they appear in are collected.  \n",
    "   The actor is assigned the mean of these values, or `None` if no valid values exist.\n",
    "\n",
    "4. **Aggregate sentiment at community level**  \n",
    "   Actor sentiment scores are grouped by community using the Louvain partition.  \n",
    "   The mean sentiment is then computed for each community.\n",
    "\n",
    "5. **Rank communities by sentiment**  \n",
    "   Communities are sorted by their mean sentiment score in descending order.\n",
    "\n",
    "6. **Link top communities with names and sentiment**  \n",
    "   The top 10 communities are selected and combined with their automatically generated names and sentiment scores into a structured list for presentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894baf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LabMT happiness dataset\n",
    "labmt = pd.read_csv(\"Data_Set_S1.txt\", sep=\"\\t\")\n",
    "\n",
    "# Create word-to-happiness dictionary\n",
    "labmt_dict = dict(zip(\n",
    "    labmt[\"word\"].astype(str).str.lower(),\n",
    "    labmt[\"happiness_average\"]\n",
    "))\n",
    "\n",
    "# Simple tokenizer for lowercase word extraction\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"[a-z']+\", str(text).lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LabMT-based sentiment score for a text\n",
    "def labmt_sentiment(text, word_dict, lens=1.0, center=5.0):\n",
    "    # Tokenize input text\n",
    "    tokens = tokenize(text)\n",
    "    \n",
    "    # Retrieve happiness scores for known words\n",
    "    scores = [word_dict[t] for t in tokens if t in word_dict]\n",
    "\n",
    "    # Remove neutral words based on threshold\n",
    "    filtered = [s for s in scores if s < center - lens or s > center + lens]\n",
    "\n",
    "    # Return None if no valid words remain\n",
    "    if not filtered:\n",
    "        return None\n",
    "\n",
    "    # Return mean sentiment score\n",
    "    return float(np.mean(filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9717ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews\n",
    "reviews_df = pd.read_csv(\"data/reviews.csv\")\n",
    "\n",
    "# Compute sentiment score for each review\n",
    "reviews_df[\"sentiment\"] = reviews_df[\"reviews\"].apply(\n",
    "    lambda text: labmt_sentiment(text, labmt_dict)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average sentiment per movie\n",
    "movie_sentiment = (\n",
    "    reviews_df.groupby(\"movieId\")[\"sentiment\"].mean().to_dict()\n",
    ")\n",
    "\n",
    "# Remove NaN values\n",
    "movie_sentiment = {m: s for m, s in movie_sentiment.items() if s == s}\n",
    "\n",
    "# Print result\n",
    "print(movie_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02397662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average sentiment per actor\n",
    "actor_sentiment = {}\n",
    "\n",
    "for actor, movies in actor_movies.items():\n",
    "    vals = []\n",
    "    for m in movies:\n",
    "        if m in movie_sentiment:\n",
    "            if not np.isnan(movie_sentiment[m]):\n",
    "                vals.append(movie_sentiment[m])\n",
    "    \n",
    "    # Assign mean sentiment or None if no values exist\n",
    "    actor_sentiment[actor] = np.mean(vals) if len(vals) > 0 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect actor sentiment scores per community\n",
    "community_sentiment = defaultdict(list)\n",
    "\n",
    "for actor, comm in partition_gcc.items():\n",
    "    if actor_sentiment[actor] is not None:\n",
    "        community_sentiment[comm].append(actor_sentiment[actor])\n",
    "\n",
    "# Compute mean sentiment per community\n",
    "community_sentiment_mean = {\n",
    "    comm: np.mean(vals) for comm, vals in community_sentiment.items()\n",
    "}\n",
    "\n",
    "# Sort communities by mean sentiment score\n",
    "sorted_sent = sorted(\n",
    "    community_sentiment_mean.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Create structured list of top communities by sentiment\n",
    "linked = []\n",
    "\n",
    "for comm, sent in sorted_sent[:10]:\n",
    "    name = community_names.get(comm, \"Unknown\")\n",
    "    linked.append({\n",
    "        \"community\": comm,\n",
    "        \"name\": name,\n",
    "        \"sentiment\": sent\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5384d2",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c06f93",
   "metadata": {},
   "source": [
    "In this section, a structural backbone of the network is extracted and visualized using the Disparity Filter method.\n",
    "\n",
    "1. **Apply the Disparity Filter**  \n",
    "   The filter is applied to the cleaned graph `H` with a chosen significance level `alpha = 0.5`.  \n",
    "   This removes statistically insignificant edges while preserving the main structural connections of the network.\n",
    "\n",
    "2. **Compute layout for visualization**  \n",
    "   A spring layout is computed for the backbone graph to position nodes in a visually interpretable way.\n",
    "\n",
    "3. **Visualize the backbone network**  \n",
    "   The filtered network is plotted with small nodes and semi-transparent edges to highlight the main connectivity structure after edge reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degree values\n",
    "degrees = [d for n, d in H.degree()]\n",
    "\n",
    "# Plot degree distribution\n",
    "plt.hist(degrees, bins=range(1, max(degrees) + 1), edgecolor=\"black\")\n",
    "plt.title(\"Degree Distribution\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "\n",
    "# Use logarithmic scale for better visibility\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a385c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort nodes by degree\n",
    "degree_sequence = sorted(H.degree(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top 5 actors by degree\n",
    "print(\"\\nTop 5 actors from degree:\")\n",
    "for actor, degree in degree_sequence[:5]:\n",
    "    print(f\"{actor}: {degree}\")\n",
    "\n",
    "# Print bottom 5 actors by degree\n",
    "print(\"\\nBottom 5 actors from degree:\")\n",
    "for actor, degree in degree_sequence[-5:]:\n",
    "    print(f\"{actor}: {degree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract network backbone using the Disparity Filter\n",
    "def disparity_filter(G, alpha=0.05):\n",
    "    backbone = nx.Graph()\n",
    "    backbone.add_nodes_from(G.nodes(data=True))\n",
    "\n",
    "    for node in G.nodes():\n",
    "        k = len(list(G.neighbors(node)))  # Node degree\n",
    "\n",
    "        # Keep all edges for nodes with degree 1\n",
    "        if k <= 1:\n",
    "            for nbr, data in G[node].items():\n",
    "                backbone.add_edge(node, nbr, **data)\n",
    "            continue\n",
    "\n",
    "        # Sum of incident edge weights\n",
    "        w_sum = sum(data[\"weight\"] for _, data in G[node].items())\n",
    "\n",
    "        for nbr, data in G[node].items():\n",
    "            w = data[\"weight\"]\n",
    "            p_ij = w / w_sum\n",
    "\n",
    "            # Disparity Filter significance test\n",
    "            alpha_ij = 1 - (1 - p_ij) ** (k - 1)\n",
    "\n",
    "            # Keep statistically significant edges\n",
    "            if alpha_ij < alpha:\n",
    "                backbone.add_edge(node, nbr, **data)\n",
    "\n",
    "    return backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the disparity filter to the graph\n",
    "backbone_H = disparity_filter(H, alpha=0.5)\n",
    "\n",
    "# Compute layout for visualization\n",
    "pos = nx.spring_layout(backbone_H, seed=42)\n",
    "\n",
    "# Plot the backbone graph\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx_nodes(backbone_H, pos, node_size=20, node_color=\"blue\")\n",
    "nx.draw_networkx_edges(backbone_H, pos, alpha=0.5)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocknet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
